global:
  ntp_servers:
    - "0.pl.pool.ntp.org"
    - "1.pl.pool.ntp.org"
  dns_servers:
    - "1.1.1.1"
    - "8.8.8.8"
    - "1.0.0.1"
    - "8.8.4.4"
k3os:
  ssh_authorized_keys:
    - ssh-rsa AAAAB3NzaC1yc2EAAAABIwAAAfsAyuh1O+SfVRgddjQ8nByz2nNzXm71sP9FAPi5G5WoJA7El3fdrOWsG5bj5kQveVZj/M4qlETajcxMLwaFQd36nS4C0WjmzfoX2i6X+iWHFH/L/ha0UaDclfYkuGdxtCeYRakJ0Xqsm0vNuMpGhIGQP8j+OSLK0Q9pnrd1md+jrxx6JS1MXM5iUh+zU4roklrGy75WQvDJcCD3miVnktJsjb2hda8cIxkRf+l88xPm5TcC82cVAWpXF91+kUVdcHiB1D13t0IDEL6BdeUHffQij1++9x6JUukIEVDoP8Ot6DC16cbma4H9ssjdc9vnTGJjsZiu7Am+BNLKD4zsMnuSqWWfPYzQuoG5HZlyonzQIYncxo98LsHLRtrEE/CWnnO+jrd8Zfe9nAatelBgVv86UHoKEPyL666nd4UxaTiyKaXfNlvHWhfgbV67ZGRTrmllfdTmu1olOIniO706L3t3V+Zu6RjN5VjYdLegKKMo2E6XwhkMCcIg9Syi95pj8x8jFoKwjvOhb1EleuZyNHNbEvosIjAy1HsUgEo5bIwiV8K0aWIQE8VVyqyhdXOpR/Gk9UtFYsACyfCX84DjQBXp7OXIkyYt9s5N15Panml2X1CXnNgsh9M60+RvJBsFj3k8M57zcc7VAarAUKnYrI59l3cCZZVLtd4aIp8= zynzel@banshee
  k3os_args:
    - "--disable"
    - "traefik"
    - "--disable"
    - "servicelb"
    - "--kube-apiserver-arg"
    - "default-unreachable-toleration-seconds=120"
    - "--kube-apiserver-arg"
    - "default-not-ready-toleration-seconds=120"
  rancher_user_password: rancher
  ntp_servers:
    - "{{ global.ntp_servers[0] }}"
    - "{{ global.ntp_servers[1] }}"
  dns_servers:
    - "{{ global.dns_servers[0] }}"
    - "{{ global.dns_servers[1] }}"
  servers:
    - ip: 10.0.120.31
      hostname: k3s-bravo
      role: master
      labels:
        video_processing: "true"
        zigbee_controller: "true"
      boot_cmds:
        - "ln -vs /usr/share/zoneinfo/Europe/Warsaw /etc/localtime"
        - "echo 'Europe/Warsaw' > /etc/timezone"
      run_cmds:
        - "connmanctl config ethernet_d8cb8ad15025_cable --ipv6 off --nameservers {{ global.dns_servers[0] }} {{ global.dns_servers[1] }} --ipv4 dhcp"
    - ip: 10.0.120.32
      hostname: k3s-charlie
      role: worker
      labels:
        video_processing: "true"
      boot_cmds:
        - "ln -vs /usr/share/zoneinfo/Europe/Warsaw /etc/localtime"
        - "echo 'Europe/Warsaw' > /etc/timezone"
      run_cmds:
        - "connmanctl config ethernet_002324acfb8a_cable --ipv6 off --nameservers {{ global.dns_servers[0] }} {{ global.dns_servers[1] }} --ipv4 dhcp"
      env:
        K3S_TOKEN: <TOKEN>
    - ip: 10.0.120.33
      hostname: k3s-delta
      role: worker
      labels:
        video_processing: "true"
        ups_controller: "true"
        samba_controller: "true"
      boot_cmds:
        - "ln -vs /usr/share/zoneinfo/Europe/Warsaw /etc/localtime"
        - "echo 'Europe/Warsaw' > /etc/timezone"
        - "mkdir /srv/samba"
      run_cmds:
        - "connmanctl config ethernet_685b35949584_cable --ipv6 off --nameservers {{ global.dns_servers[0] }} {{ global.dns_servers[1] }} --ipv4 dhcp"
      env:
        K3S_TOKEN: <TOKEN>
      write_files:
        - content: |
            /dev/cdrom   /media/cdrom iso9660 noauto,ro 0 0
            /dev/usbdisk /media/usb   vfat    noauto,ro 0 0
            /dev/sdb     /srv/samba   ext4    defaults  0 2
          owner: root
          path: /etc/fstab
          permissions: '0644'
  global_write_files:
    - content: |
        [global]
        OfflineMode=false
        TimeUpdates=manual
        TimezoneUpdates=manual
        [Wired]
        Enable=true
        Tethering=false
        [Bluetooth]
        Enable=false
        Tethering=false
      owner: root
      path: /var/lib/connman/settings
      permissions: '0600'
    - content: |
        apiVersion: v1
        kind: ConfigMap
        metadata:
          name: coredns
          namespace: kube-system
        data:
          Corefile: |
            home {{ manifest.domain }}:53 {
              errors
              health
              ready
              forward . {{ manifest.vips[0].ip }}:53
              loop
              reload
              loadbalance
            }
            .:53 {
                errors
                health
                ready
                kubernetes cluster.local in-addr.arpa ip6.arpa {
                  pods insecure
                  fallthrough in-addr.arpa ip6.arpa
                }
                hosts /etc/coredns/NodeHosts {
                  ttl 60
                  reload 15s
                  fallthrough
                }
                prometheus :9153
                forward . /etc/resolv.conf
                cache 30
                loop
                reload
                loadbalance
            }
      path: /var/lib/rancher/k3s/server/manifests/z-custom-coredns.yaml
      permissions: "0600"
manifest:
  domain: <DOMAIN>
  mail: <MAIL>
  timezone: Europe/Warsaw
  vips:
    - ip: 10.0.10.40
    - ip: 10.0.10.41
    - ip: 10.0.10.42
    - ip: 10.0.10.43
    - ip: 10.0.10.44
    - ip: 10.0.10.45
    - ip: 10.0.10.46
  backup:
    restic:
      repo: "<RESTIC_REPO>"
      password: "<RESTIC_PASSWORD>"
      repo_samba: "<RESTIC_REPO>"
  versions:
    http_server:
      repository: halverneus/static-file-server
      tag: v1.8.4
    chrony:
      repository: geoffh1977/chrony
      tag: latest
    restic:
      repository: restic/restic
      tag: 0.12.0
    samba:
      repository: mbentley/timemachine
      tag: smb-20210731
    nut:
      chart: 4.1.0
      image:
        repository: ghcr.io/k8s-at-home/network-ups-tools
        tag: v2.7.4-2481-g4b243d10
    zigbee2mqtt:
      chart: 8.3.0
      image:
        repository: koenkk/zigbee2mqtt
        tag: 1.20.0
    esphome:
      image:
        repository: esphome/esphome
        tag: 1.19.4
    grafana:
      chart: 6.13.10
      image:
        repository: grafana/grafana
        tag: 8.0.5
    homeassistant:
      chart: 9.3.0
      image:
        repository: homeassistant/home-assistant
        tag: 2021.7.4
    nodered:
      chart: 8.1.0
      image:
        repository: nodered/node-red
        tag: 2.0.4-12
    unifi:
      chart: 2.0.4
      image:
        repository: jacobalberty/unifi
        tag: v6.2.26
    recorder:
      web:
        repository: "{{ manifest.versions.http_server.repository }}"
        tag: "{{ manifest.versions.http_server.tag }}"
      cleanup:
        repository: "{{ manifest.versions.ubuntu.repository }}"
        tag: "{{ manifest.versions.ubuntu.tag }}"
      repository: bkupidura/recorder
      tag: latest
    metallb:
      chart: 0.10.2
      controller:
        repository: metallb/controller
        tag: v0.10.2
      speaker:
        repository: metallb/speaker
        tag: v0.10.2
    longhorn:
      chart: 1.1.2
    ubuntu:
      repository: ubuntu
      tag: focal-20210713
    blocky:
      chart: 7.1.0
      image:
        repository: spx01/blocky
        tag: v0.14
    traefik:
      chart: 10.1.1
      image:
        repository: traefik
        tag: 2.4.12
    certmanager:
      chart: v1.4.0
      image:
        repository: quay.io/jetstack/cert-manager-controller
        tag: v1.4.0
    mosquitto:
      chart: 3.3.0
      image:
        repository: eclipse-mosquitto
        tag: 2.0.11
    prometheus:
      chart: 14.4.1
    frigate:
      repository: blakeblackshear/frigate
      tag: 0.8.4-amd64
  samba:
    password: "<SAMBA_PASSWORD>"
  nut:
    auth:
      admin: "<ADMIN_PASSWORD>"
      hass: "<HASS_PASSWORD>"
  grafana:
    password: "<GRAFANA_PASSWORD>"
  prometheus:
    serverfiles:
      alerting_rules.yml:
        groups:
          - name: infra
            rules:
              - alert: 1MLoadHigh
                expr: node_load1 > machine_cpu_cores * 1.3
                for: 2m
                labels:
                  service: system
                annotations:
                  summary: "{% raw %}High load avg from 1m on {{ $labels.kubernetes_node }}{% endraw %}"
              - alert: 5MLoadHigh
                expr: node_load5 > machine_cpu_cores * 0.8
                for: 5m
                labels:
                  service: system
                annotations:
                  summary: "{% raw %}High load avg from 5m on {{ $labels.kubernetes_node }}{% endraw %}"
              - alert: HighMemory
                expr: node_memory_MemFree_bytes / node_memory_MemTotal_bytes > 0.7
                for: 5m
                labels:
                  service: system
                annotations:
                  summary: "{% raw %}High memory usage on {{ $labels.kubernetes_node }}{% endraw %}"
              - alert: CertInvalidShortly
                expr: (certmanager_certificate_expiration_timestamp_seconds - time()) / 60 / 60 / 24 < 5
                labels:
                  service: certmanager
                annotations:
                  summary: "Certificate will expire soon"
              - alert: BlockyErrorsIncreasing
                expr: increase(blocky_error_total[10m]) > 0
                labels:
                  service: blocky
                annotations:
                  summary: "{% raw %}Errors increasing on {{ $labels.kubernetes_pod_name }}{% endraw %}"
              - alert: MetalLbBGPDown
                expr: max_over_time(metallb_bgp_session_up[1d]) - metallb_bgp_session_up != 0
                labels:
                  service: metallb
                annotations:
                  summary: "{% raw %}BGP sessions down on {{ $labels.instance }}{% endraw %}"
              - alert: LowDisk
                expr: min by (device, instance) (node_filesystem_free_bytes{device=~"/dev/[a-z]d[a-z][0-9]*"} / node_filesystem_size_bytes) < 0.3
                labels:
                  service: system
                annotations:
                  summary: "{% raw %}High disk usage on {{ $labels.kubernetes_node }} on {{ $labels.device }} mounted as {{ $labels.mountpoint }}{% endraw %}"
          - name: prometheus
            rules:
              - alert: InstanceDown
                expr: up == 0
                for: 5m
                labels:
                  service: prometheus
                annotations:
                  summary: "{% raw %}Prometheus instance {{ $labels.instance }} is down for job {{ $labels.job }}{% endraw %}"
          - name: k8s
            rules:
              - alert: JobIsNotCompleted
                expr: kube_job_spec_completions - kube_job_status_succeeded > 0
                for: 2h
                labels:
                  service: k8s
                annotations:
                  summary: "{% raw %}Job {{ $labels.job_name }} is not done for last 2h{% endraw %}"
              - alert: JobFailed
                expr: kube_job_status_failed > 0
                for: 5m
                labels:
                  service: k8s
                annotations:
                  summary: "{% raw %}Job {{ $labels.job_name }} is in failed state{% endraw %}"
              - alert: PodsRestarts
                expr: delta(kube_pod_container_status_restarts_total[30m]) > 2
                for: 1m
                labels:
                  service: k8s
                annotations:
                  summary: "{% raw %}Pod {{ $labels.pod }} was restarted more than 2 times in last 30m{% endraw %}"
              - alert: PodsWaiting
                expr: kube_pod_container_status_waiting > 0
                for: 10m
                labels:
                  service: k8s
                annotations:
                  summary: "{% raw %}Pod {{ $labels.pod }} is in waiting state for last 10m{% endraw %}"
              - alert: DaemonSetUnavailable
                expr: kube_daemonset_status_number_unavailable != 0
                for: 10m
                labels:
                  service: k8s
                annotations:
                  summary: "{% raw %}Daemonset {{ $labels.daemonset }} has unavailable copies for last 10m{% endraw %}"
              - alert: WrongNumberOfDaemonSet
                expr: (kube_daemonset_status_current_number_scheduled / kube_daemonset_status_desired_number_scheduled != 1) or (kube_daemonset_status_number_ready / kube_daemonset_status_desired_number_scheduled != 1)
                for: 10m
                labels:
                  service: k8s
                annotations:
                  summary: "{% raw %}Daemonset {{ $labels.daemonset }} has wrong number of copies{% endraw %}"
              - alert: EndpointNotReady
                expr: kube_endpoint_address_not_ready != 0
                for: 5m
                labels:
                  service: k8s
                annotations:
                  summary: "{% raw %}Endpoint {{ $labels.endpoint }} is not ready for last 5m{% endraw %}"
              - alert: PendingPods
                expr: scheduler_pending_pods{job="kubernetes-nodes"} > 0
                for: 5m
                labels:
                  service: k8s
                annotations:
                  summary: "{% raw %}Pending pods on {{ $labels.kubernetes_io_hostname }} in queue {{ $labels.queue }}{% endraw %}"
              - alert: RunningPodsFlapping
                expr: abs(delta(kubelet_running_pods{job="kubernetes-nodes"}[1h])) > 2
                for: 20m
                labels:
                  service: k8s
                annotations:
                  summary: "{% raw %}Flapping pods on {{ $labels.kubernetes_io_hostname }}{% endraw %}"
          - name: longhorn
            rules:
              - alert: WrongVolumeRobustness
                expr: longhorn_volume_robustness != 1
                for: 10m
                labels:
                  service: longhorn
                annotations:
                  summary: "{% raw %}Volume {{ $labels.volume }} is not healthy {% endraw %}"
              - alert: LowDisk
                expr: longhorn_disk_usage_bytes / longhorn_disk_capacity_bytes > 0.7
                labels:
                  service: longhorn
                annotations:
                  summary: "{% raw %}High disk usage on {{ $labels.node }}{% endraw %}"
              - alert: LonghornNodeDown
                expr: longhorn_node_status != 1
                labels:
                  service: longhorn
                annotations:
                  summary: "{% raw %}Node {{ $labels.node }} is unhealthy ({{ $labels.condition }}){% endraw %}"
    extrascrape:
      - job_name: 'home-assistant'
        scrape_interval: 10s
        metrics_path: /api/prometheus
        bearer_token: '<TOKEN>'
        scheme: http
        static_configs:
          - targets: ['home-assistant.smart-home:8123']
        metric_relabel_configs:
          - regex: 'friendly_name'
            action: 'labeldrop'
          - source_labels: [entity]
            regex: '.*\.(.*)'
            replacement: '${1}'
            target_label: 'entity_name'
  metallb:
    config:
      peers:
         - peer-address: 10.0.120.1
           peer-asn: 64501
           my-asn: 64500
      address-pools:
        - name: default
          protocol: bgp
          addresses:
            - 10.0.10.0/24
  recorder:
    volume:
      size: 30Gi
    secret:
      mqtt_password: "<PASSWORD>"
      ssh_string: "<SSH_STRING>"
      id_rsa: |
        -----BEGIN RSA PRIVATE KEY-----
        <KEY>
        -----END RSA PRIVATE KEY-----
  frigate:
    mqtt:
      password: "<PASSWORD>"
    volume:
      size: 1Gi
    rtsp:
      user: "<USERNAME>"
      password: "<PASSWORD>"
    cameras:
      cam1_kitchen:
        ffmpeg:
          inputs:
            - path: "rtsp://{{ manifest.frigate.rtsp.user }}:{{ manifest.frigate.rtsp.password }}@10.0.150.25:554/Streaming/Channels/102?transportmode=unicast&profile=Profile_1&tcp"
              roles:
                - detect
        width: 480
        height: 640
        fps: 5
        snapshots:
          enabled: true
          bounding_box: true
          crop: true
          retain:
            default: 1
            objects:
              person: 2
        rtmp:
          enabled: false
      cam2_office:
        ffmpeg:
          inputs:
            - path: "rtsp://{{ manifest.frigate.rtsp.user }}:{{ manifest.frigate.rtsp.password }}@10.0.150.26:554/Streaming/Channels/102?transportmode=unicast&profile=Profile_1&tcp"
              roles:
                - detect
        width: 640
        height: 480
        fps: 5
        snapshots:
          enabled: true
          bounding_box: true
          crop: true
          retain:
            default: 1
            objects:
              person: 2
        rtmp:
          enabled: false
  mosquitto:
    htaccess:
     - <USER>:<PASSWORD>
     - <USER2>:<PASSWORD>
     - <USER3>:<PASSWORD>
  traefik:
    tls:
      secret_name: "{{ manifest.domain | replace('.', '-') }}-tls"
    htaccess:
      - '<USER>:<PASSWORD>'
  blocky:
    blocking:
      client_group:
        default:
          - malware
          - ads
          - privacy
      blacklist:
        malware:
          - http://hole.cert.pl/domains/domains_hosts.txt
          - https://raw.githubusercontent.com/PolishFiltersTeam/KADhosts/master/KADhosts.txt
          - https://curben.gitlab.io/malware-filter/urlhaus-filter-hosts-online.txt
          - https://raw.githubusercontent.com/MajkiIT/polish-ads-filter/master/polish-pihole-filters/KADhosts.txt
          - https://blocklistproject.github.io/Lists/abuse.txt
          - https://blocklistproject.github.io/Lists/malware.txt
          - https://blocklistproject.github.io/Lists/phishing.txt
          - https://blocklistproject.github.io/Lists/ransomware.txt
        ads:
          - https://pgl.yoyo.org/adservers/serverlist.php?hostformat=hosts&showintro=1&mimetype=plaintext
          - https://raw.githubusercontent.com/MajkiIT/polish-ads-filter/master/polish-pihole-filters/Ad_filter_list_by_Disconnect.txt
          - https://raw.githubusercontent.com/MajkiIT/polish-ads-filter/master/polish-pihole-filters/hostfile.txt
          - https://raw.githubusercontent.com/MajkiIT/polish-ads-filter/master/polish-pihole-filters/adguard_mobile_host.txt
          - https://raw.githubusercontent.com/MajkiIT/polish-ads-filter/master/polish-pihole-filters/adservers.txt
          - https://blocklistproject.github.io/Lists/ads.txt
          - https://blocklistproject.github.io/Lists/fraud.txt
          - https://blocklistproject.github.io/Lists/scam.txt
        privacy:
          - https://raw.githubusercontent.com/MajkiIT/polish-ads-filter/master/polish-pihole-filters/NoTrack_Tracker_Blocklist.txt
          - https://raw.githubusercontent.com/MajkiIT/polish-ads-filter/master/polish-pihole-filters/easy_privacy_host.txt
          - https://blocklistproject.github.io/Lists/tracking.txt
    conditional:
      mapping:
        home: udp:10.0.120.1
    custom_dns:
      mapping:
        "grafana.{{ manifest.domain }}": "{{ manifest.vips[2].ip }}"
        "longhorn.{{ manifest.domain }}": "{{ manifest.vips[2].ip }}"
        "traefik.{{ manifest.domain }}": "{{ manifest.vips[2].ip }}"
        "prometheus.{{ manifest.domain }}": "{{ manifest.vips[2].ip }}"
        "mqtt.{{ manifest.domain }}": "{{ manifest.vips[3].ip }}"
        "frigate.{{ manifest.domain }}": "{{ manifest.vips[2].ip }}"
        "recorder.{{ manifest.domain }}": "{{ manifest.vips[2].ip }}"
        "unifi.{{ manifest.domain }}": "{{ manifest.vips[2].ip }}"
        "node-red.{{ manifest.domain }}": "{{ manifest.vips[2].ip }}"
        "z2m.{{ manifest.domain }}": "{{ manifest.vips[2].ip }}"
        "{{ manifest.domain }}": "{{ manifest.vips[2].ip }}"
  r53:
    access_key: <ACCESS_KEY>
    secret_key: <SECRET_KEY>
