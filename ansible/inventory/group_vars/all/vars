ansible_user: ansible
global:
  k8s:
    storage:
      service: rook-ceph
      class:
        default:
          name: ceph-filesystem
          mode: ReadWriteMany
  domain: placeholder
  mail: placeholder
  vips:
    - ip: 10.0.10.40
    - ip: 10.0.10.41
    - ip: 10.0.10.42
    - ip: 10.0.10.43
    - ip: 10.0.10.44
    - ip: 10.0.10.45
    - ip: 10.0.10.46
  timezone: "Europe/Warsaw"
  ntp:
    servers:
    - "0.pl.pool.ntp.org"
    - "1.pl.pool.ntp.org"
  dns:
    servers:
    - "1.1.1.1"
    - "8.8.8.8"
    - "1.0.0.1"
    - "8.8.4.4"
os:
  ssh_keys:
    - user: root
      key: "ssh-rsa AAAAB3NzaC1yc2EAAAABIwAAAfsAyuh1O+SfVRgddjQ8nByz2nNzXm71sP9FAPi5G5WoJA7El3fdrOWsG5bj5kQveVZj/M4qlETajcxMLwaFQd36nS4C0WjmzfoX2i6X+iWHFH/L/ha0UaDclfYkuGdxtCeYRakJ0Xqsm0vNuMpGhIGQP8j+OSLK0Q9pnrd1md+jrxx6JS1MXM5iUh+zU4roklrGy75WQvDJcCD3miVnktJsjb2hda8cIxkRf+l88xPm5TcC82cVAWpXF91+kUVdcHiB1D13t0IDEL6BdeUHffQij1++9x6JUukIEVDoP8Ot6DC16cbma4H9ssjdc9vnTGJjsZiu7Am+BNLKD4zsMnuSqWWfPYzQuoG5HZlyonzQIYncxo98LsHLRtrEE/CWnnO+jrd8Zfe9nAatelBgVv86UHoKEPyL666nd4UxaTiyKaXfNlvHWhfgbV67ZGRTrmllfdTmu1olOIniO706L3t3V+Zu6RjN5VjYdLegKKMo2E6XwhkMCcIg9Syi95pj8x8jFoKwjvOhb1EleuZyNHNbEvosIjAy1HsUgEo5bIwiV8K0aWIQE8VVyqyhdXOpR/Gk9UtFYsACyfCX84DjQBXp7OXIkyYt9s5N15Panml2X1CXnNgsh9M60+RvJBsFj3k8M57zcc7VAarAUKnYrI59l3cCZZVLtd4aIp8= zynzel@banshee"
    - user: ansible
      key: "ssh-rsa AAAAB3NzaC1yc2EAAAABIwAAAfsAyuh1O+SfVRgddjQ8nByz2nNzXm71sP9FAPi5G5WoJA7El3fdrOWsG5bj5kQveVZj/M4qlETajcxMLwaFQd36nS4C0WjmzfoX2i6X+iWHFH/L/ha0UaDclfYkuGdxtCeYRakJ0Xqsm0vNuMpGhIGQP8j+OSLK0Q9pnrd1md+jrxx6JS1MXM5iUh+zU4roklrGy75WQvDJcCD3miVnktJsjb2hda8cIxkRf+l88xPm5TcC82cVAWpXF91+kUVdcHiB1D13t0IDEL6BdeUHffQij1++9x6JUukIEVDoP8Ot6DC16cbma4H9ssjdc9vnTGJjsZiu7Am+BNLKD4zsMnuSqWWfPYzQuoG5HZlyonzQIYncxo98LsHLRtrEE/CWnnO+jrd8Zfe9nAatelBgVv86UHoKEPyL666nd4UxaTiyKaXfNlvHWhfgbV67ZGRTrmllfdTmu1olOIniO706L3t3V+Zu6RjN5VjYdLegKKMo2E6XwhkMCcIg9Syi95pj8x8jFoKwjvOhb1EleuZyNHNbEvosIjAy1HsUgEo5bIwiV8K0aWIQE8VVyqyhdXOpR/Gk9UtFYsACyfCX84DjQBXp7OXIkyYt9s5N15Panml2X1CXnNgsh9M60+RvJBsFj3k8M57zcc7VAarAUKnYrI59l3cCZZVLtd4aIp8= zynzel@banshee"

k3s:
  version: "v1.22.3+k3s1"
  args:
    - "--disable"
    - "traefik"
    - "--disable"
    - "servicelb"
    - "--kube-apiserver-arg"
    - "default-unreachable-toleration-seconds=120"
    - "--kube-apiserver-arg"
    - "default-not-ready-toleration-seconds=120"
    - "--kube-apiserver-arg"
    - "feature-gates=EphemeralContainers=true"
    - "--kubelet-arg"
    - "allowed-unsafe-sysctls=net.ipv4.tcp_keepalive_intvl,net.ipv4.tcp_keepalive_probes,net.ipv4.tcp_keepalive_time"
    - "--kubelet-arg"
    - "image-gc-high-threshold=60"
    - "--kubelet-arg"
    - "image-gc-low-threshold=50"
    - "--kubelet-arg"
    - "node-status-max-images=-1"
manifest:
  backup:
    restic:
      repo: placeholder
      password: placeholder
      repo_samba: placeholder
  versions:
    smsgammu:
      repository: pajikos/sms-gammu-gateway
      tag: 1.1
    kubefledged:
      chart: v0.8.2
    descheduler:
      chart: 0.22.0
    loki:
      chart: 2.5.0
    ceph:
      chart: v1.7.7
      image:
        repository: quay.io/ceph/ceph
        tag: v16.2.6-20210927
    reflector:
      chart: 6.0.21
    deconz:
      firmware:
        url: https://deconz.dresden-elektronik.de/deconz-firmware/
        name: deCONZ_ConBeeII_0x266f0700.bin.GCF
      image:
        repository: marthoc/deconz
        tag: 2.12.03
    k8s_dashboard:
      chart: 5.0.4
    http_server:
      repository: halverneus/static-file-server
      tag: v1.8.4
    chrony:
      repository: bkupidura/chrony
      tag: 16102021
    restic:
      repository: restic/restic
      tag: 0.12.1
    samba:
      repository: mbentley/timemachine
      tag: smb-20211107
    nut:
      chart: 6.0.1
      image:
        repository: ghcr.io/k8s-at-home/network-ups-tools
        tag: v2.7.4-2486-gaa0b3d1d
    zigbee2mqtt:
      chart: 9.0.1
      image:
        repository: koenkk/zigbee2mqtt
        tag: 1.22.0
    esphome:
      chart: 8.0.1
      image:
        repository: esphome/esphome
        tag: 2021.10.3
    grafana:
      chart: 6.17.5
      image:
        repository: grafana/grafana
        tag: 8.2.3
    homeassistant:
      chart: 11.0.4
      image:
        repository: homeassistant/home-assistant
        tag: 2021.10.4
    nodered:
      chart: 9.0.1
      image:
        repository: nodered/node-red
        tag: 2.1.3-14
    unifi:
      chart: 4.3.0
      image:
        repository: jacobalberty/unifi
        tag: v6.4.54
    recorder:
      repository: bkupidura/recorder
      tag: 0.0.2
    metallb:
      chart: 0.11.0
      controller:
        repository: metallb/controller
        tag: v0.11.0
      speaker:
        repository: metallb/speaker
        tag: v0.11.0
    longhorn:
      chart: 1.2.0
    ubuntu:
      repository: ubuntu
      tag: focal-20211006
    blocky:
      chart: 9.0.3
      image:
        repository: spx01/blocky
        tag: v0.16
    traefik:
      chart: 10.6.1
      image:
        repository: traefik
        tag: v2.5.3
    certmanager:
      chart: v1.6.1
      image:
        repository: quay.io/jetstack/cert-manager-controller
        tag: v1.6.1
    mosquitto:
      chart: 4.0.1
      image:
        repository: eclipse-mosquitto
        tag: 2.0.13
    prometheus:
      chart: 14.11.1
    frigate:
      chart: 6.1.1
      image:
        repository: blakeblackshear/frigate
        tag: 0.9.4-amd64
  sms:
    password: placeholder
  samba:
    password: placeholder
  nut:
    auth:
      admin: placeholder
      hass: placeholder
  grafana:
    password: placeholder
  alertmanager:
    receivers:
      - name: 'default-receiver'
    inhibit_rules:
      - source_match:
          severity: critical
        target_match:
          severity: warning
        equal:
          - service
      - source_match:
          severity: warning
        target_match:
          severity: info
        equal:
          - service
  prometheus:
    serverfiles:
      alerting_rules.yml:
        groups:
          - name: infra
            rules:
              - alert: TraefikServiceErrors
                expr: sum by (service, method, protocol, code) (delta(traefik_service_requests_total{code=~"(4|5).."}[15m])) > 20
                for: 5m
                labels:
                  service: traefik
                  severity: warning
                annotations:
                  summary: "{%- raw -%}Traefik service requests error increase for {{ $labels.service }}, code {{ $labels.code }}{%- endraw -%}"
              - alert: OOMKill
                expr: delta(node_vmstat_oom_kill[30m]) > 0
                for: 1m
                labels:
                  service: system
                  severity: warning
                annotations:
                  summary: "{%- raw -%}Out of memory kill observed on {{ $labels.kubernetes_node }}{%- endraw -%}"
              - alert: HighMaxErrorTimeDrift
                expr: node_timex_maxerror_seconds > 0.5
                for: 5m
                labels:
                  service: system
                  severity: warning
                annotations:
                  summary: "{%- raw -%}High max error time drift observed on {{ $labels.kubernetes_node }}{%- endraw -%}"
              - alert: 1MLoadHigh
                expr: node_load1 > on (kubernetes_node) (count by (kubernetes_node) (node_cpu_seconds_total{mode="system"})) * 1.5
                for: 10m
                labels:
                  service: system
                  severity: warning
                annotations:
                  summary: "{%- raw -%}High load avg {{ humanize $value }} from 1m on {{ $labels.kubernetes_node }}{%- endraw -%}"
              - alert: 15MLoadHigh
                expr: node_load15 > on (kubernetes_node) (count by (kubernetes_node) (node_cpu_seconds_total{mode="system"})) * 0.95
                for: 30m
                labels:
                  service: system
                  severity: warning
                annotations:
                  summary: "{%- raw -%}High load avg {{ humanize $value }} from 15m on {{ $labels.kubernetes_node }}{%- endraw -%}"
              - alert: HighMemory
                expr: node_memory_MemAvailable_bytes / node_memory_MemTotal_bytes < 0.1
                for: 10m
                labels:
                  service: system
                  severity: warning
                annotations:
                  summary: "{%- raw -%}Low free memory {{ $value | humanizePercentage }} on {{ $labels.kubernetes_node }}{%- endraw -%}"
              - alert: CertInvalidShortly
                expr: (certmanager_certificate_expiration_timestamp_seconds - time()) / 60 / 60 / 24 < 29
                labels:
                  service: certmanager
                  severity: info
                annotations:
                  summary: "Certificate will expire soon"
              - alert: BlockyErrorsIncreasing
                expr: increase(blocky_error_total[10m]) > 10
                labels:
                  service: blocky
                  severity: info
                annotations:
                  summary: "{%- raw -%}Errors increasing on {{ $labels.kubernetes_pod_name }}{%- endraw -%}"
              - alert: MetalLbBGPDown
                expr: max_over_time(metallb_bgp_session_up[1d]) - metallb_bgp_session_up != 0
                labels:
                  service: metallb
                  severity: warning
                annotations:
                  summary: "{%- raw -%}BGP sessions down on {{ $labels.instance }}{%- endraw -%}"
              - alert: LowDisk
                expr: min by (device, instance) (node_filesystem_free_bytes{device=~"/dev/[a-z]d[a-z][0-9]*"} / node_filesystem_size_bytes) < 0.3
                labels:
                  service: system
                  severity: warning
                annotations:
                  summary: "{%- raw -%}High disk usage on {{ $labels.kubernetes_node }} on {{ $labels.device }} mounted as {{ $labels.mountpoint }}{%- endraw -%}"
          - name: prometheus
            rules:
              - alert: InstanceDown
                expr: up == 0
                for: 5m
                labels:
                  service: prometheus
                  severity: warning
                annotations:
                  summary: "{%- raw -%}Prometheus instance {{ $labels.instance }} is down for job {{ $labels.job }}{%- endraw -%}"
              - alert: PrometheusBadConfig
                expr: max_over_time(prometheus_config_last_reload_successful[5m]) == 0
                for: 10m
                labels:
                  service: prometheus
                  severity: critical
                annotations:
                  summary: "{%- raw -%}Failed Prometheus configuration reload{%- endraw -%}"
              - alert: PrometheusErrorSendingAlertsToAnyAlertmanager
                expr: min without(alertmanager) (rate(prometheus_notifications_errors_total[5m]) /
                    rate(prometheus_notifications_sent_total[5m])) * 100 > 3
                for: 15m
                labels:
                  service: prometheus
                  severity: critical
                annotations:
                  summary: "{%- raw -%}Prometheus encounters more than 3% errors sending alerts to any Alertmanager{%- endraw -%}"
              - alert: PrometheusNotIngestingSamples
                expr: rate(prometheus_tsdb_head_samples_appended_total[5m]) <= 0
                for: 10m
                labels:
                  service: prometheus
                  severity: warning
                annotations:
                  summary: "{%- raw -%}Prometheus is not ingesting samples{%- endraw -%}"
              - alert: PrometheusRuleFailures
                expr: increase(prometheus_rule_evaluation_failures_total[5m]) > 0
                for: 15m
                labels:
                  service: prometheus
                  severity: critical
                annotations:
                  summary: "{%- raw -%}Prometheus is failing rule evaluations{%- endraw -%}"
          - name: k8s
            rules:
              - alert: VolumeUsageHigh
                expr: kubelet_volume_stats_used_bytes{job="kubernetes-nodes"} / kubelet_volume_stats_capacity_bytes > 0.75
                for: 10m
                labels:
                  service: k8s
                  severity: warning
                annotations:
                  summary: "{%- raw -%}Volume for PVC {{ $labels.persistentvolumeclaim }} is using more than 75% os available storage{%- endraw -%}"
              - alert: StatefulSetUnhealthy
                expr: "kube_statefulset_status_replicas_ready / kube_statefulset_status_replicas < 0.7"
                for: 10m
                labels:
                  service: k8s
                  severity: warning
                annotations:
                  summary: "{%- raw -%}StatefulSet {{ $labels.statefulset }} have less than 70% of available replicas ready{%- endraw -%}"
              - alert: DeploymentUnhealthy
                expr: "kube_deployment_status_replicas_available / kube_deployment_status_replicas < 0.7"
                for: 10m
                labels:
                  service: k8s
                  severity: warning
                annotations:
                  summary: "{%- raw -%}Deployment {{ $labels.deployment }} have less than 70% of available replicas ready{%- endraw -%}"
              - alert: HighCPUHVLimit
                expr: sum by (kubernetes_io_hostname) (container_spec_cpu_quota{container!=""} / 100) /
                  on (kubernetes_io_hostname) label_replace(kube_node_status_allocatable{resource="cpu"} * 1000, "kubernetes_io_hostname", "$1", "node", "(.+)")
                    > 1
                for: 30m
                labels:
                  service: k8s
                  severity: info
                annotations:
                  summary: "{%- raw -%}PODs running on {{ $labels.kubernetes_io_hostname }} have higher CPU limits than total HV capacity{%- endraw -%}"
              - alert: HighMemoryHVUsage
                expr: sum by (kubernetes_io_hostname) (container_memory_working_set_bytes{container!=""}) /
                  on (kubernetes_io_hostname) label_replace(kube_node_status_allocatable{resource="memory"}, "kubernetes_io_hostname", "$1", "node", "(.+)")
                  > 0.8
                for: 30m
                labels:
                  service: k8s
                  severity: warning
                annotations:
                  summary: "{%- raw -%}PODs running on {{ $labels.kubernetes_io_hostname }} are using 80% of HV total memory{%- endraw -%}"
              - alert: HighMemoryHVLimit
                expr: sum by (kubernetes_io_hostname) (container_spec_memory_limit_bytes{container!=""}) /
                  on (kubernetes_io_hostname) label_replace(kube_node_status_allocatable{resource="memory"}, "kubernetes_io_hostname", "$1", "node", "(.+)")
                  > 1
                for: 30m
                labels:
                  service: k8s
                  severity: info
                annotations:
                  summary: "{%- raw -%}PODs running on {{ $labels.kubernetes_io_hostname }} have higher memory limits than 90% of HV total memory{%- endraw -%}"
              - alert: HighMemoryPodUsage
                expr: max by (pod, namespace) (container_memory_working_set_bytes{container!=""} / container_spec_memory_limit_bytes < Inf) > 0.90
                for: 30m
                labels:
                  service: k8s
                  severity: warning
                annotations:
                  summary: "{%- raw -%}POD {{ $labels.pod }} is using {{ $value | humanizePercentage }} of memory limit{%- endraw -%}"
              - alert: ClusterRunningDifferentK8sVersionComponets
                expr: count (count by (git_version) (kubernetes_build_info)) > 1
                for: 10m
                labels:
                  service: k8s
                  severity: warning
                annotations:
                  summary: Some components running in cluster have different k8s git_version than others
              - alert: NodeNotReady
                expr: kube_node_status_condition{condition="Ready",status="true"} != 1
                for: 5m
                labels:
                  service: k8s
                  severity: critical
                annotations:
                  summary: "{%- raw -%}{{ $labels.node }} is unready{%- endraw -%}"
              - alert: NodeReadinesFlapping
                expr: sum by (node) (changes(kube_node_status_condition{status="true",condition="Ready"}[10m])) > 2
                for: 5m
                labels:
                  service: k8s
                  severity: warning
                annotations:
                  summary: "{%- raw -%}{{ $labels.node }} is readiness is flapping{%- endraw -%}"
              - alert: PodCPUThrotling
                expr: sum(increase(container_cpu_cfs_throttled_periods_total{container!="", }[5m])) by (container, pod, namespace) / sum(increase(container_cpu_cfs_periods_total{}[5m])) by (container, pod, namespace) > 0.5
                for: 15m
                labels:
                  service: k8s
                  severity: warning
                annotations:
                  summary: "{%- raw -%}{{ $value | humanizePercentage }} throttling of CPU in namespace {{ $labels.namespace }} for container {{ $labels.container }} in pod {{ $labels.pod }}{%- endraw -%}"
              - alert: PodCPUThrotling
                expr: sum(increase(container_cpu_cfs_throttled_periods_total{container!="", }[5m])) by (container, pod, namespace) / sum(increase(container_cpu_cfs_periods_total{}[5m])) by (container, pod, namespace) > 0.3
                for: 20m
                labels:
                  service: k8s
                  severity: info
                annotations:
                  summary: "{%- raw -%}{{ $value | humanizePercentage }} throttling of CPU in namespace {{ $labels.namespace }} for container {{ $labels.container }} in pod {{ $labels.pod }}{%- endraw -%}"
              - alert: NodeUnschedulable
                expr: kube_node_spec_unschedulable != 0
                for: 2m
                labels:
                  service: k8s
                  severity: critical
                annotations:
                  summary: "{%- raw -%}K8s node {{ $labels.node }} is unschedulable{%- endraw -%}"
              - alert: JobIsNotCompleted
                expr: kube_job_spec_completions - kube_job_status_succeeded > 0
                for: 2h
                labels:
                  service: k8s
                  severity: info
                annotations:
                  summary: "{%- raw -%}Job {{ $labels.job_name }} is not done for last 2h{%- endraw -%}"
              - alert: JobFailed
                expr: kube_job_status_failed > 0
                for: 5m
                labels:
                  service: k8s
                  severity: warning
                annotations:
                  summary: "{%- raw -%}Job {{ $labels.job_name }} is in failed state{%- endraw -%}"
              - alert: PodsRestarts
                expr: delta(kube_pod_container_status_restarts_total[30m]) > 2
                for: 1m
                labels:
                  service: k8s
                  severity: warning
                annotations:
                  summary: "{%- raw -%}Pod {{ $labels.pod }} was restarted {{ humanize $value }} times in last 30m{%- endraw -%}"
              - alert: PodsWaiting
                expr: max by (pod) (kube_pod_container_status_waiting) > 0
                for: 10m
                labels:
                  service: k8s
                  severity: info
                annotations:
                  summary: "{%- raw -%}Pod {{ $labels.pod }} is in waiting state for last 10m{%- endraw -%}"
              - alert: DaemonSetUnavailable
                expr: kube_daemonset_status_number_unavailable != 0
                for: 10m
                labels:
                  service: k8s
                  severity: warning
                annotations:
                  summary: "{%- raw -%}Daemonset {{ $labels.daemonset }} has unavailable copies for last 10m{%- endraw -%}"
              - alert: WrongNumberOfDaemonSet
                expr: (kube_daemonset_status_current_number_scheduled / kube_daemonset_status_desired_number_scheduled != 1) or (kube_daemonset_status_number_ready / kube_daemonset_status_desired_number_scheduled != 1)
                for: 10m
                labels:
                  service: k8s
                  severity: warning
                annotations:
                  summary: "{%- raw -%}Daemonset {{ $labels.daemonset }} has wrong number of copies{%- endraw -%}"
              - alert: EndpointNotReady
                expr: kube_endpoint_address_not_ready != 0
                for: 5m
                labels:
                  service: k8s
                  severity: info
                annotations:
                  summary: "{%- raw -%}Endpoint {{ $labels.endpoint }} is not ready for last 5m{%- endraw -%}"
              - alert: PendingPods
                expr: scheduler_pending_pods{job="kubernetes-nodes"} > 0
                for: 5m
                labels:
                  service: k8s
                  severity: warning
                annotations:
                  summary: "{%- raw -%}Pending pods on {{ $labels.kubernetes_io_hostname }} in queue {{ $labels.queue }}{%- endraw -%}"
              - alert: RunningPodsFlapping
                expr: abs(delta(kubelet_running_pods{job="kubernetes-nodes"}[1h])) > 2
                for: 20m
                labels:
                  service: k8s
                  severity: warning
                annotations:
                  summary: "{%- raw -%}Flapping pods on {{ $labels.kubernetes_io_hostname }}{%- endraw -%}"
          - name: ceph
            rules:
              - alert: CephMdsMissingReplicas
                expr: sum(ceph_mds_metadata == 1) < 2
                for: 5m
                labels:
                  service: ceph
                  severity: warning
                annotations:
                   summary: Insufficient replicas for storage metadata service
              - alert: CephMonQuorumAtRisk
                expr: count(ceph_mon_quorum_status == 1) <= (floor(count(ceph_mon_metadata) / 2) + 1)
                for: 5m
                labels:
                  service: ceph
                  severity: warning
                annotations:
                   summary: Storage quorum at risk
              - alert: CephOSDCriticallyFull
                expr: (ceph_osd_metadata * on (ceph_daemon) group_right(device_class,hostname) (ceph_osd_stat_bytes_used / ceph_osd_stat_bytes)) > 0.80
                for: 5m
                labels:
                  service: ceph
                  severity: warning
                annotations:
                   summary: "{%- raw -%}Back-end storage device is critically full ({{ $value | humanizePercentage }}) on {{ $labels.ceph_daemon }}{%- endraw -%}"
              - alert: CephOSDFlapping
                expr: changes(ceph_osd_up[5m]) > 3
                for: 5m
                labels:
                  service: ceph
                  severity: warning
                annotations:
                   summary: "{%- raw -%}Ceph storage osd flapping on {{ $labels.ceph_daemon }}{%- endraw -%}"
              - alert: CephOSDSlowOps
                expr: ceph_healthcheck_slow_ops > 0
                for: 1m
                labels:
                  service: ceph
                  severity: warning
                annotations:
                   summary: "{%- raw -%}Slow ops detected in ceph cluster{%- endraw -%}"
              - alert: CephPGNotClean
                expr: ceph_pg_clean != ceph_pg_total
                for: 1h
                labels:
                  service: ceph
                  severity: warning
                annotations:
                   summary: "{%- raw -%}Not clean PGs detected in cluster{%- endraw -%}"
              - alert: CephPGNotActive
                expr: ceph_pg_active != ceph_pg_total
                for: 1h
                labels:
                  service: ceph
                  severity: warning
                annotations:
                   summary: "{%- raw -%}Not active PGs detected in cluster{%- endraw -%}"
              - alert: CephPGUndersized
                expr: ceph_pg_undersized > 0
                for: 30m
                labels:
                  service: ceph
                  severity: warning
                annotations:
                   summary: "{%- raw -%}PGs data recovery is slow{%- endraw -%}"
              - alert: CephPGInconsistent
                expr: ceph_pg_inconsistent > 0
                for: 30m
                labels:
                  service: ceph
                  severity: warning
                annotations:
                   summary: "{%- raw -%}Inconsistent PGs detected in cluster{%- endraw -%}"
              - alert: MissingCephHealthStatus
                expr: absent(ceph_health_status) == 1
                for: 10m
                labels:
                  service: ceph
                  severity: warning
                annotations:
                  summary: "Missing ceph_health_status metric"
              - alert: CephClusterErrorState
                expr: ceph_health_status > 1
                for: 5m
                labels:
                  service: ceph
                  severity: warning
                annotations:
                   summary: "{%- raw -%}Storage cluster is in error state{%- endraw -%}"
              - alert: CephClusterWarningState
                expr: ceph_health_status == 1
                for: 20m
                labels:
                  service: ceph
                  severity: warning
                annotations:
                   summary: "{%- raw -%}Storage cluster is in warning state{%- endraw -%}"
              - alert: CephServiceVersionMismatch
                expr: count(count by(ceph_version) ({__name__=~"^ceph_(osd|mgr|mds|mon)_metadata$"})) > 1
                for: 10m
                labels:
                  service: ceph
                  severity: warning
                annotations:
                   summary: "{%- raw -%}There are multiple versions of services running{%- endraw -%}"
              - alert: CephClusterCriticallyFull
                expr: ceph_cluster_total_used_raw_bytes / ceph_cluster_total_bytes > 0.75
                for: 30m
                labels:
                  service: ceph
                  severity: warning
                annotations:
                   summary: "{%- raw -%}Storage cluster is critically full{%- endraw -%}"
              - alert: CephClusterReadOnly
                expr: ceph_cluster_total_used_raw_bytes / ceph_cluster_total_bytes >= 0.85
                for: 1m
                labels:
                  service: ceph
                  severity: critical
                annotations:
                   summary: "{%- raw -%}Storage cluster is in read only mode{%- endraw -%}"
          - name: longhorn
            rules:
              - alert: WrongVolumeRobustness
                expr: longhorn_volume_robustness != 1
                for: 10m
                labels:
                  service: longhorn
                  severity: warning
                annotations:
                  summary: "{%- raw -%}Volume {{ $labels.volume }} is not healthy{%- endraw -%}"
              - alert: LowDisk
                expr: longhorn_disk_usage_bytes / longhorn_disk_capacity_bytes > 0.7
                labels:
                  service: longhorn
                  severity: info
                annotations:
                  summary: "{%- raw -%}High disk usage on {{ $labels.node }}{%- endraw -%}"
              - alert: LonghornNodeDown
                expr: longhorn_node_status != 1
                labels:
                  service: longhorn
                  severity: critical
                annotations:
                  summary: "{%- raw -%}Node {{ $labels.node }} is unhealthy ({{ $labels.condition }}){%- endraw -%}"
    extrascrape:
      - job_name: 'home-assistant'
        scrape_interval: 10s
        metrics_path: /api/prometheus
        bearer_token: 'placeholder'
        scheme: http
        static_configs:
          - targets: ['home-assistant.smart-home:8123']
        metric_relabel_configs:
          - regex: 'friendly_name'
            action: 'labeldrop'
          - source_labels: [entity]
            regex: '.*\.(.*)'
            replacement: '${1}'
            target_label: 'entity_name'
  metallb:
    config:
      peers:
        - peer-address: 10.0.120.1
          peer-asn: 64501
          my-asn: 64500
      address-pools:
        - name: default
          protocol: bgp
          addresses:
            - 10.0.10.0/24
  recorder:
    volume:
      size: 30Gi
    secret:
      mqtt_password: placeholder
      ssh_string: placeholder
      id_rsa: |
        -----BEGIN RSA PRIVATE KEY-----
        placeholder
        -----END RSA PRIVATE KEY-----
  frigate:
    mqtt:
      password: placeholder
    volume:
      size: 1Gi
    cameras:
      cam1_kitchen:
        ffmpeg:
          inputs:
            - path: "rtsp://placeholder:placeholder@10.0.150.25:554/Streaming/Channels/102?transportmode=unicast&profile=Profile_1&tcp"
              roles:
                - detect
        detect:
          width: 480
          height: 640
          fps: 5
        snapshots:
          enabled: true
          bounding_box: true
          crop: true
          retain:
            default: 1
            objects:
              person: 2
        rtmp:
          enabled: false
        record:
          enabled: false
      cam2_office:
        ffmpeg:
          inputs:
            - path: "rtsp://placeholder:placeholder@10.0.150.26:554/Streaming/Channels/102?transportmode=unicast&profile=Profile_1&tcp"
              roles:
                - detect
        detect:
          width: 640
          height: 480
          fps: 5
        snapshots:
          enabled: true
          bounding_box: true
          crop: true
          timestamp: true
          retain:
            default: 1
            objects:
              person: 2
        rtmp:
          enabled: false
        record:
          enabled: false
  mosquitto:
    htaccess:
      - 'placeholder'
  traefik:
    htaccess:
      - 'placeholder'
  blocky:
    blocking:
      client_group:
        default:
          - malware
          - ads
          - privacy
      blacklist:
        malware:
          - http://hole.cert.pl/domains/domains_hosts.txt
          - https://raw.githubusercontent.com/PolishFiltersTeam/KADhosts/master/KADhosts.txt
          - https://curben.gitlab.io/malware-filter/urlhaus-filter-hosts-online.txt
          - https://raw.githubusercontent.com/MajkiIT/polish-ads-filter/master/polish-pihole-filters/KADhosts.txt
          - https://blocklistproject.github.io/Lists/abuse.txt
          - https://blocklistproject.github.io/Lists/malware.txt
          - https://blocklistproject.github.io/Lists/phishing.txt
          - https://blocklistproject.github.io/Lists/ransomware.txt
        ads:
          - https://pgl.yoyo.org/adservers/serverlist.php?hostformat=hosts&showintro=1&mimetype=plaintext
          - https://raw.githubusercontent.com/MajkiIT/polish-ads-filter/master/polish-pihole-filters/Ad_filter_list_by_Disconnect.txt
          - https://raw.githubusercontent.com/MajkiIT/polish-ads-filter/master/polish-pihole-filters/hostfile.txt
          - https://raw.githubusercontent.com/MajkiIT/polish-ads-filter/master/polish-pihole-filters/adguard_mobile_host.txt
          - https://raw.githubusercontent.com/MajkiIT/polish-ads-filter/master/polish-pihole-filters/adservers.txt
          - https://blocklistproject.github.io/Lists/ads.txt
          - https://blocklistproject.github.io/Lists/fraud.txt
          - https://blocklistproject.github.io/Lists/scam.txt
        privacy:
          - https://raw.githubusercontent.com/MajkiIT/polish-ads-filter/master/polish-pihole-filters/NoTrack_Tracker_Blocklist.txt
          - https://raw.githubusercontent.com/MajkiIT/polish-ads-filter/master/polish-pihole-filters/easy_privacy_host.txt
          - https://blocklistproject.github.io/Lists/tracking.txt
    conditional:
      mapping:
        home: udp:10.0.120.1
    custom_dns:
      mapping:
      - domain: "esphome.{{ global.domain }}"
        ip: "{{ global.vips[2].ip }}"
      - domain: "k8s.{{ global.domain }}"
        ip: "{{ global.vips[2].ip }}"
      - domain: "grafana.{{ global.domain }}"
        ip: "{{ global.vips[2].ip }}"
      - domain: "storage.{{ global.domain }}"
        ip: "{{ global.vips[2].ip }}"
      - domain: "traefik.{{ global.domain }}"
        ip: "{{ global.vips[2].ip }}"
      - domain: "alertmanager.{{ global.domain }}"
        ip: "{{ global.vips[2].ip }}"
      - domain: "prometheus.{{ global.domain }}"
        ip: "{{ global.vips[2].ip }}"
      - domain: "mqtt.{{ global.domain }}"
        ip: "{{ global.vips[3].ip }}"
      - domain: "frigate.{{ global.domain }}"
        ip: "{{ global.vips[2].ip }}"
      - domain: "recorder.{{ global.domain }}"
        ip: "{{ global.vips[2].ip }}"
      - domain: "unifi.{{ global.domain }}"
        ip: "{{ global.vips[2].ip }}"
      - domain: "node-red.{{ global.domain }}"
        ip: "{{ global.vips[2].ip }}"
      - domain: "z2m.{{ global.domain }}"
        ip: "{{ global.vips[2].ip }}"
      - domain: "{{ global.domain }}"
        ip: "{{ global.vips[2].ip }}"
  r53:
    access_key: placeholder
    secret_key: placeholder
