alerts:
  - name: restic
    rules:
      - alert: ResticNoNewBackup
        expr: delta(rest_server_blob_write_bytes_total{type="snapshots"}[5d]) <= 0
        for: 1d
        labels:
          service: restic
          severity: warning
        annotations:
          summary: "{%- raw -%}No new backups found for repo {{ $labels.repo }}{%- endraw -%}"
  - name: infra
    rules:
      - alert: SystemNodeRebooted
        expr: time() - node_boot_time_seconds < 600
        labels:
          service: system
          severity: warning
        annotations:
          summary: "{%- raw -%}{{ $labels.node }} was rebooted{%- endraw -%}"
      - alert: TraefikServiceErrors4XX
        expr: sum by (service, protocol) (delta(traefik_service_requests_total{code=~"4.."}[5m])) / sum by(service, protocol) (delta(traefik_service_requests_total{code!~"(4|5).."}[5m])) > 0.3
        for: 10m
        labels:
          service: traefik
          severity: warning
        annotations:
          summary: "{%- raw -%}Traefik service requests error (4XX) increase for {{ $labels.protocol }}/{{ $labels.service }}{%- endraw -%}"
      - alert: TraefikServiceErrors5XX
        expr: sum by (service, protocol) (delta(traefik_service_requests_total{code=~"5.."}[5m])) / sum by(service, protocol) (delta(traefik_service_requests_total{code!~"(4|5).."}[5m])) > 0.1
        for: 10m
        labels:
          service: traefik
          severity: warning
        annotations:
          summary: "{%- raw -%}Traefik service requests error (5XX) increase for {{ $labels.protocol }}/{{ $labels.service }}{%- endraw -%}"
      - alert: SystemOOMKill
        expr: delta(node_vmstat_oom_kill[30m]) > 0
        for: 1m
        labels:
          service: system
          severity: warning
        annotations:
          summary: "{%- raw -%}Out of memory kill observed on {{ $labels.node }}{%- endraw -%}"
      - alert: SystemHighMaxErrorTimeDrift
        expr: node_timex_maxerror_seconds > 0.5
        for: 5m
        labels:
          service: system
          severity: warning
        annotations:
          summary: "{%- raw -%}High max error time drift observed on {{ $labels.node }}{%- endraw -%}"
      - alert: System1MLoadHigh
        expr: node_load1 > on (node) (count by (node) (node_cpu_seconds_total{mode="system"})) * 1.5
        for: 10m
        labels:
          service: system
          severity: warning
        annotations:
          summary: "{%- raw -%}High load avg {{ humanize $value }} from 1m on {{ $labels.node }}{%- endraw -%}"
      - alert: System15MLoadHigh
        expr: node_load15 > on (node) (count by (node) (node_cpu_seconds_total{mode="system"})) * 0.95
        for: 60m
        labels:
          service: system
          severity: warning
        annotations:
          summary: "{%- raw -%}High load avg {{ humanize $value }} from 15m on {{ $labels.node }}{%- endraw -%}"
      - alert: SystemHighMemory
        expr: node_memory_MemAvailable_bytes / node_memory_MemTotal_bytes < 0.1
        for: 10m
        labels:
          service: system
          severity: warning
        annotations:
          summary: "{%- raw -%}Low free memory {{ $value | humanizePercentage }} on {{ $labels.node }}{%- endraw -%}"
      - alert: CertInvalidShortly
        expr: (certmanager_certificate_expiration_timestamp_seconds - time()) / 60 / 60 / 24 < 29
        labels:
          service: certmanager
          severity: info
        annotations:
          summary: "Certificate will expire soon"
      - alert: BlockyErrorsIncreasing
        expr: increase(blocky_error_total[10m]) > 10
        labels:
          service: blocky
          severity: info
        annotations:
          summary: "{%- raw -%}Errors increasing on {{ $labels.pod }}{%- endraw -%}"
      - alert: MetalLbBGPDown
        expr: max_over_time(metallb_bgp_session_up[1d]) - metallb_bgp_session_up != 0
        labels:
          service: metallb
          severity: warning
        annotations:
          summary: "{%- raw -%}BGP sessions down on {{ $labels.instance }}{%- endraw -%}"
      - alert: SystemLowDisk
        expr: min by (device, instance) (node_filesystem_free_bytes{device=~"/dev/[a-z]d[a-z][0-9]*"} / node_filesystem_size_bytes) < 0.3
        labels:
          service: system
          severity: warning
        annotations:
          summary: "{%- raw -%}High disk usage on {{ $labels.node }} on {{ $labels.device }} mounted as {{ $labels.mountpoint }}{%- endraw -%}"
  - name: prometheus
    rules:
      - alert: PrometheusInstanceDown
        expr: up == 0
        for: 5m
        labels:
          service: prometheus
          severity: warning
        annotations:
          summary: "{%- raw -%}Prometheus instance {{ $labels.instance }} is down for job {{ $labels.job }}{%- endraw -%}"
      - alert: PrometheusBadConfig
        expr: max_over_time(prometheus_config_last_reload_successful[5m]) == 0
        for: 10m
        labels:
          service: prometheus
          severity: critical
        annotations:
          summary: "{%- raw -%}Failed Prometheus configuration reload{%- endraw -%}"
      - alert: PrometheusErrorSendingAlertsToAnyAlertmanager
        expr: min without(alertmanager) (rate(prometheus_notifications_errors_total[5m]) /
            rate(prometheus_notifications_sent_total[5m])) * 100 > 3
        for: 15m
        labels:
          service: prometheus
          severity: critical
        annotations:
          summary: "{%- raw -%}Prometheus encounters more than 3% errors sending alerts to any Alertmanager{%- endraw -%}"
      - alert: PrometheusNotIngestingSamples
        expr: rate(prometheus_tsdb_head_samples_appended_total[5m]) <= 0
        for: 10m
        labels:
          service: prometheus
          severity: warning
        annotations:
          summary: "{%- raw -%}Prometheus is not ingesting samples{%- endraw -%}"
      - alert: PrometheusRuleFailures
        expr: increase(prometheus_rule_evaluation_failures_total[5m]) > 0
        for: 15m
        labels:
          service: prometheus
          severity: critical
        annotations:
          summary: "{%- raw -%}Prometheus is failing rule evaluations{%- endraw -%}"
  - name: k8s
    rules:
      - alert: K8sVolumeUsageHigh
        expr: kubelet_volume_stats_used_bytes{job="kubernetes-nodes"} / kubelet_volume_stats_capacity_bytes > 0.75
        for: 10m
        labels:
          service: k8s
          severity: warning
        annotations:
          summary: "{%- raw -%}Volume for PVC {{ $labels.persistentvolumeclaim }} is using more than 75% os available storage{%- endraw -%}"
      - alert: K8sStatefulSetUnhealthy
        expr: "kube_statefulset_status_replicas_ready / kube_statefulset_status_replicas < 0.7"
        for: 10m
        labels:
          service: k8s
          severity: warning
        annotations:
          summary: "{%- raw -%}StatefulSet {{ $labels.statefulset }} have less than 70% of available replicas ready{%- endraw -%}"
      - alert: K8sDeploymentUnhealthy
        expr: "kube_deployment_status_replicas_available / kube_deployment_status_replicas < 0.7"
        for: 10m
        labels:
          service: k8s
          severity: warning
        annotations:
          summary: "{%- raw -%}Deployment {{ $labels.deployment }} have less than 70% of available replicas ready{%- endraw -%}"
      - alert: K8sHighCPUHVLimit
        expr: sum by (kubernetes_io_hostname) (container_spec_cpu_quota{container!=""} / 100) /
          on (kubernetes_io_hostname) label_replace(kube_node_status_allocatable{resource="cpu"} * 1000, "kubernetes_io_hostname", "$1", "node", "(.+)")
            > 1
        for: 30m
        labels:
          service: k8s
          severity: info
        annotations:
          summary: "{%- raw -%}PODs running on {{ $labels.kubernetes_io_hostname }} have higher CPU limits than total HV capacity{%- endraw -%}"
      - alert: K8sHighMemoryHVUsage
        expr: sum by (kubernetes_io_hostname) (container_memory_working_set_bytes{container!=""}) /
          on (kubernetes_io_hostname) label_replace(kube_node_status_allocatable{resource="memory"}, "kubernetes_io_hostname", "$1", "node", "(.+)")
          > 0.8
        for: 30m
        labels:
          service: k8s
          severity: warning
        annotations:
          summary: "{%- raw -%}PODs running on {{ $labels.kubernetes_io_hostname }} are using 80% of HV total memory{%- endraw -%}"
      - alert: K8sHighMemoryHVLimit
        expr: sum by (kubernetes_io_hostname) (container_spec_memory_limit_bytes{container!=""}) /
          on (kubernetes_io_hostname) label_replace(kube_node_status_allocatable{resource="memory"}, "kubernetes_io_hostname", "$1", "node", "(.+)")
          > 1
        for: 30m
        labels:
          service: k8s
          severity: info
        annotations:
          summary: "{%- raw -%}PODs running on {{ $labels.kubernetes_io_hostname }} have higher memory limits than 90% of HV total memory{%- endraw -%}"
      - alert: K8sHighMemoryPodUsage
        expr: max by (pod, namespace) (container_memory_working_set_bytes{container!=""} / container_spec_memory_limit_bytes < Inf) > 0.95
        for: 30m
        labels:
          service: k8s
          severity: warning
        annotations:
          summary: "{%- raw -%}POD {{ $labels.pod }} is using {{ $value | humanizePercentage }} of memory limit{%- endraw -%}"
      - alert: K8sClusterRunningDifferentK8sVersionComponets
        expr: count (count by (git_version) (kubernetes_build_info)) > 1
        for: 10m
        labels:
          service: k8s
          severity: warning
        annotations:
          summary: Some components running in cluster have different k8s git_version than others
      - alert: K8sNodeNotReady
        expr: kube_node_status_condition{condition="Ready",status="true"} != 1
        for: 5m
        labels:
          service: k8s
          severity: critical
        annotations:
          summary: "{%- raw -%}{{ $labels.node }} is unready{%- endraw -%}"
      - alert: K8sNodeReadinesFlapping
        expr: sum by (node) (changes(kube_node_status_condition{status="true",condition="Ready"}[10m])) > 2
        for: 5m
        labels:
          service: k8s
          severity: warning
        annotations:
          summary: "{%- raw -%}{{ $labels.node }} is readiness is flapping{%- endraw -%}"
      - alert: K8sPodCPUThrotling
        expr: sum(increase(container_cpu_cfs_throttled_periods_total{container!~"^(frigate|)$"}[5m])) by (container, pod, namespace) / sum(increase(container_cpu_cfs_periods_total{}[5m])) by (container, pod, namespace) > 0.5
        for: 15m
        labels:
          service: k8s
          severity: warning
        annotations:
          summary: "{%- raw -%}{{ $value | humanizePercentage }} throttling of CPU in namespace {{ $labels.namespace }} for container {{ $labels.container }} in pod {{ $labels.pod }}{%- endraw -%}"
      - alert: K8sPodCPUThrotling
        expr: sum(increase(container_cpu_cfs_throttled_periods_total{container!~"^(frigate|)$"}[5m])) by (container, pod, namespace) / sum(increase(container_cpu_cfs_periods_total{}[5m])) by (container, pod, namespace) > 0.3
        for: 30m
        labels:
          service: k8s
          severity: info
        annotations:
          summary: "{%- raw -%}{{ $value | humanizePercentage }} throttling of CPU in namespace {{ $labels.namespace }} for container {{ $labels.container }} in pod {{ $labels.pod }}{%- endraw -%}"
      - alert: K8sNodeUnschedulable
        expr: kube_node_spec_unschedulable != 0
        for: 2m
        labels:
          service: k8s
          severity: critical
        annotations:
          summary: "{%- raw -%}K8s node {{ $labels.node }} is unschedulable{%- endraw -%}"
      - alert: K8sJobIsNotCompleted
        expr: kube_job_spec_completions - kube_job_status_succeeded > 0
        for: 3h
        labels:
          service: k8s
          severity: info
        annotations:
          summary: "{%- raw -%}Job {{ $labels.job_name }} is not done for last 3h{%- endraw -%}"
      - alert: K8sJobFailed
        expr: kube_job_status_failed > 0
        for: 5m
        labels:
          service: k8s
          severity: warning
        annotations:
          summary: "{%- raw -%}Job {{ $labels.job_name }} is in failed state{%- endraw -%}"
      - alert: K8sPodsRestarts
        expr: delta(kube_pod_container_status_restarts_total[30m]) > 2
        for: 1m
        labels:
          service: k8s
          severity: warning
        annotations:
          summary: "{%- raw -%}Pod {{ $labels.pod }} was restarted {{ humanize $value }} times in last 30m{%- endraw -%}"
      - alert: K8sPodsWaiting
        expr: max by (pod) (kube_pod_container_status_waiting) > 0
        for: 10m
        labels:
          service: k8s
          severity: info
        annotations:
          summary: "{%- raw -%}Pod {{ $labels.pod }} is in waiting state for last 10m{%- endraw -%}"
      - alert: K8sDaemonSetUnavailable
        expr: kube_daemonset_status_number_unavailable != 0
        for: 10m
        labels:
          service: k8s
          severity: warning
        annotations:
          summary: "{%- raw -%}Daemonset {{ $labels.daemonset }} has unavailable copies for last 10m{%- endraw -%}"
      - alert: K8sWrongNumberOfDaemonSet
        expr: (kube_daemonset_status_current_number_scheduled / kube_daemonset_status_desired_number_scheduled != 1) or (kube_daemonset_status_number_ready / kube_daemonset_status_desired_number_scheduled != 1)
        for: 10m
        labels:
          service: k8s
          severity: warning
        annotations:
          summary: "{%- raw -%}Daemonset {{ $labels.daemonset }} has wrong number of copies{%- endraw -%}"
      - alert: K8sEndpointNotReady
        expr: kube_endpoint_address_not_ready / (kube_endpoint_address_not_ready + kube_endpoint_address_available) > 0.4
        for: 5m
        labels:
          service: k8s
          severity: info
        annotations:
          summary: "{%- raw -%}Endpoint {{ $labels.endpoint }} has more than 40% of not ready members for last 5m{%- endraw -%}"
      - alert: K8sPendingPods
        expr: scheduler_pending_pods{job="kubernetes-nodes"} > 0
        for: 5m
        labels:
          service: k8s
          severity: warning
        annotations:
          summary: "{%- raw -%}Pending pods on {{ $labels.kubernetes_io_hostname }} in queue {{ $labels.queue }}{%- endraw -%}"
      - alert: K8sRunningPodsFlapping
        expr: abs(delta(kubelet_running_pods{job="kubernetes-nodes"}[1h])) > 2
        for: 20m
        labels:
          service: k8s
          severity: warning
        annotations:
          summary: "{%- raw -%}Flapping pods on {{ $labels.kubernetes_io_hostname }}{%- endraw -%}"
  - name: ceph
    enabled: false
    rules:
      - alert: CephMdsMissingReplicas
        expr: sum(ceph_mds_metadata == 1) < 2
        for: 5m
        labels:
          service: ceph
          severity: warning
        annotations:
           summary: Insufficient replicas for storage metadata service
      - alert: CephMonQuorumAtRisk
        expr: count(ceph_mon_quorum_status == 1) <= (floor(count(ceph_mon_metadata) / 2) + 1)
        for: 5m
        labels:
          service: ceph
          severity: warning
        annotations:
           summary: Storage quorum at risk
      - alert: CephOSDCriticallyFull
        expr: (ceph_osd_metadata * on (ceph_daemon) group_right(device_class,hostname) (ceph_osd_stat_bytes_used / ceph_osd_stat_bytes)) > 0.80
        for: 5m
        labels:
          service: ceph
          severity: warning
        annotations:
           summary: "{%- raw -%}Back-end storage device is critically full ({{ $value | humanizePercentage }}) on {{ $labels.ceph_daemon }}{%- endraw -%}"
      - alert: CephOSDFlapping
        expr: changes(ceph_osd_up[5m]) > 3
        for: 5m
        labels:
          service: ceph
          severity: warning
        annotations:
           summary: "{%- raw -%}Ceph storage osd flapping on {{ $labels.ceph_daemon }}{%- endraw -%}"
      - alert: CephOSDSlowOps
        expr: ceph_healthcheck_slow_ops > 0
        for: 1m
        labels:
          service: ceph
          severity: warning
        annotations:
           summary: "{%- raw -%}Slow ops detected in ceph cluster{%- endraw -%}"
      - alert: CephPGNotClean
        expr: ceph_pg_clean != ceph_pg_total
        for: 1h
        labels:
          service: ceph
          severity: warning
        annotations:
           summary: "{%- raw -%}Not clean PGs detected in cluster{%- endraw -%}"
      - alert: CephPGNotActive
        expr: ceph_pg_active != ceph_pg_total
        for: 1h
        labels:
          service: ceph
          severity: warning
        annotations:
           summary: "{%- raw -%}Not active PGs detected in cluster{%- endraw -%}"
      - alert: CephPGUndersized
        expr: ceph_pg_undersized > 0
        for: 30m
        labels:
          service: ceph
          severity: warning
        annotations:
           summary: "{%- raw -%}PGs data recovery is slow{%- endraw -%}"
      - alert: CephPGInconsistent
        expr: ceph_pg_inconsistent > 0
        for: 30m
        labels:
          service: ceph
          severity: warning
        annotations:
           summary: "{%- raw -%}Inconsistent PGs detected in cluster{%- endraw -%}"
      - alert: CephMissingHealthStatus
        expr: absent(ceph_health_status) == 1
        for: 10m
        labels:
          service: ceph
          severity: warning
        annotations:
          summary: "Missing ceph_health_status metric"
      - alert: CephClusterErrorState
        expr: ceph_health_status > 1
        for: 5m
        labels:
          service: ceph
          severity: warning
        annotations:
           summary: "{%- raw -%}Storage cluster is in error state{%- endraw -%}"
      - alert: CephClusterWarningState
        expr: ceph_health_status == 1
        for: 20m
        labels:
          service: ceph
          severity: warning
        annotations:
           summary: "{%- raw -%}Storage cluster is in warning state{%- endraw -%}"
      - alert: CephServiceVersionMismatch
        expr: count(count by(ceph_version) ({__name__=~"^ceph_(osd|mgr|mds|mon)_metadata$"})) > 1
        for: 10m
        labels:
          service: ceph
          severity: warning
        annotations:
           summary: "{%- raw -%}There are multiple versions of services running{%- endraw -%}"
      - alert: CephClusterCriticallyFull
        expr: ceph_cluster_total_used_raw_bytes / ceph_cluster_total_bytes > 0.75
        for: 30m
        labels:
          service: ceph
          severity: warning
        annotations:
           summary: "{%- raw -%}Storage cluster is critically full{%- endraw -%}"
      - alert: CephClusterReadOnly
        expr: ceph_cluster_total_used_raw_bytes / ceph_cluster_total_bytes >= 0.85
        for: 1m
        labels:
          service: ceph
          severity: critical
        annotations:
           summary: "{%- raw -%}Storage cluster is in read only mode{%- endraw -%}"
  - name: longhorn
    rules:
      - alert: LonghornWrongVolumeRobustness
        expr: longhorn_volume_robustness > 1
        for: 10m
        labels:
          service: longhorn
          severity: warning
        annotations:
          summary: "{%- raw -%}Volume {{ $labels.volume }} is not healthy{%- endraw -%}"
      - alert: LonghornHighDiskUsage
        expr: longhorn_disk_usage_bytes / longhorn_disk_capacity_bytes > 0.7
        labels:
          service: longhorn
          severity: info
        annotations:
          summary: "{%- raw -%}High disk usage on {{ $labels.node }}{%- endraw -%}"
      - alert: LonghornNodeDown
        expr: longhorn_node_status != 1
        labels:
          service: longhorn
          severity: critical
        annotations:
          summary: "{%- raw -%}Node {{ $labels.node }} is unhealthy ({{ $labels.condition }}){%- endraw -%}"
  - name: recorder
    rules:
      - alert: RecorderWorkersHanging
        expr: recorder_workers > 0
        for: 30m
        labels:
          service: recorder
          severity: warning
        annotations:
          summary: "{%- raw -%}Recorder worker {{ $labels.service }} is running for more than 30m{%- endraw -%}"
      - alert: RecorderErrors
        expr: delta(recorder_errors_total[5m]) > 0
        for: 1m
        labels:
          service: recorder
          severity: warning
        annotations:
          summary: "{%- raw -%}Recorder errors observed for {{ $labels.service }} in last 5m{%- endraw -%}"
  - name: broker-ha
    rules:
      - alert: BrokerWrongClusterMembers
        expr: broker_cluster_members != 3
        for: 3m
        labels:
          service: broker-ha
          severity: warning
        annotations:
          summary: "{%- raw -%}Broker-ha {{ $labels.pod }} has wrong number of cluster members{%- endraw -%}"
      - alert: BrokerUnhealthy
        expr: broker_cluster_member_health > 0
        for: 2m
        labels:
          service: broker-ha
          severity: warning
        annotations:
          summary: "{%- raw -%}Broker-ha {{ $labels.pod }} is unhealthy {%- endraw -%}"
      - alert: BrokerFromClusterQueueHigh
        expr: broker_cluster_mqtt_publish_from_cluster > 0
        for: 5m
        labels:
          service: broker-ha
          severity: warning
        annotations:
          summary: "{%- raw -%}Broker-ha {{ $labels.pod }} is unable to process messages from cluster{%- endraw -%}"
      - alert: BrokerToClusterQueueHigh
        expr: broker_cluster_mqtt_publish_to_cluster > 0
        for: 5m
        labels:
          service: broker-ha
          severity: warning
        annotations:
          summary: "{%- raw -%}Broker-ha {{ $labels.pod }} is unable to process messages to cluster{%- endraw -%}"
      - alert: BrokerPublishDroppedHigh
        expr: broker_publish_dropped > 0
        for: 5m
        labels:
          service: broker-ha
          severity: warning
        annotations:
          summary: "{%- raw -%}Broker-ha {{ $labels.pod }} starts dropping publish messages{%- endraw -%}"
      - alert: BrokerInFlightHigh
        expr: broker_inflight_messages > 0
        for: 5m
        labels:
          service: broker-ha
          severity: warning
        annotations:
          summary: "{%- raw -%}Broker-ha {{ $labels.pod }} starts reporting in-flight messages{%- endraw -%}"
      - alert: BrokerRetainedMessagesMismatch
        expr: broker_retained_messages != scalar(max(broker_retained_messages))
        for: 5m
        labels:
          service: broker-ha
          severity: warning
        annotations:
          summary: "{%- raw -%}Broker-ha {{ $labels.pod }} have different number of retained messages than other cluster members{%- endraw -%}"
  - name: mosquitto
    enabled: false
    rules:
      - alert: MosquittoNoClients
        expr: broker_clients_connected == 0
        for: 5m
        labels:
          service: mosquitto
          severity: warning
        annotations:
          summary: "{%- raw -%}Mosquitto broker {{ $labels.pod }} has no client connected{%- endraw -%}"
      - alert: MosquittoPublishDropped
        expr: broker_load_publish_dropped_15min > 0
        for: 5m
        labels:
          service: mosquitto
          severity: warning
        annotations:
          summary: "{%- raw -%}Mosquitto broker {{ $labels.pod }} started dropping publish messages{%- endraw -%}"
      - alert: MosquittoMessagesSentDifferent
        expr: broker_load_messages_sent_1min / avg_over_time(broker_load_messages_sent_1min[15m]) > 2 or
          broker_load_messages_sent_1min / avg_over_time(broker_load_messages_sent_1min[15m]) < 0.5
        for: 10m
        labels:
          service: mosquitto
          severity: warning
        annotations:
          summary: "{%- raw -%}Mosquitto broker {{ $labels.pod }} start sending different ammount of messages ({{ $value | humanizePercentage }}) than in last 15m{%- endraw -%}"
      - alert: MosquittoMessagesReceiveDifferent
        expr: broker_load_messages_received_1min / avg_over_time(broker_load_messages_received_1min[15m]) > 2 or
          broker_load_messages_received_1min / avg_over_time(broker_load_messages_received_1min[15m]) < 0.5
        for: 10m
        labels:
          service: mosquitto
          severity: warning
        annotations:
          summary: "{%- raw -%}Mosquitto broker {{ $labels.pod }} start receiving different ammount of messages ({{ $value | humanizePercentage }}) than in last 15m{%- endraw -%}"
  - name: mysql
    rules:
      - alert: MysqlWrongBufferPoolUsage
        expr: delta(mysql_global_status_innodb_buffer_pool_reads[5m]) / delta(mysql_global_status_innodb_buffer_pool_read_requests[5m]) > 0.03
        for: 15m
        labels:
          service: mysql
          severity: warning
        annotations:
          summary: "{%- raw -%}Mysql wrong innodb buffer pool reads, check https://mariadb.com/kb/en/innodb-buffer-pool/#innodb_buffer_pool_size{%- endraw -%}"
      - alert: MysqlDown
        expr: mysql_up == 0
        for: 1m
        labels:
          service: mysql
          severity: critical
        annotations:
          summary: "{%- raw -%}Mysql server is down{%- endraw -%}"
      - alert: MysqlTooManyConnections
        expr: max_over_time(mysql_global_status_threads_connected[5m]) / mysql_global_variables_max_connections > 0.8
        for: 5m
        labels:
          service: mysql
          severity: warning
        annotations:
          summary: "{%- raw -%}Mysql server is using more than {{ $value | humanizePercentage }} of all available connections{%- endraw -%}"
      - alert: MysqlSlowQueries
        expr: increase(mysql_global_status_slow_queries[5m]) > 0
        for: 10m
        labels:
          service: mysql
          severity: warning
        annotations:
          summary: "{%- raw -%}Slow queries observed on Mysql{%- endraw -%}"
      - alert: MysqlInnodbLogWaits
        expr: rate(mysql_global_status_innodb_log_waits[15m]) > 10
        for: 2m
        labels:
          service: mysql
          severity: warning
        annotations:
          summary: "{%- raw -%}Mysql InnoDB log waits{%- endraw -%}"
      - alert: MysqlNoFreePages
        expr: delta(mysql_global_status_innodb_buffer_pool_wait_free[5m]) / 300 > 2
        for: 5m
        labels:
          service: mysql
          severity: warning
        annotations:
          summary: "{%- raw -%}No free pages in buffer pool{%- endraw -%}"
